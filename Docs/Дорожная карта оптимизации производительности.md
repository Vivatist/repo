# Дорожная карта оптимизации производительности NovaVPN

> Дата создания: 14.02.2026
> Последнее обновление: 14.02.2026
> Статус: этапы 0–4.5 завершены, бенчмарк-инструмент создан (режим rtt + throughput)

---

## Принципы

Все оптимизации подчиняются приоритетам проекта:
1. **Маскировка** — ни одно изменение не должно ухудшать stealth
2. **Производительность** — цель этой дорожной карты
3. **Безопасность** — шифрование неприкосновенно (оно же часть маскировки)

**Шифрование нельзя убрать/ослабить** — ChaCha20 создаёт high-entropy данные внутри TLS record, что критично для маскировки. Без шифрования DPI обнаружит structured IPv4 headers по паттерну энтропии.

---

## Профиль нагрузки (на 1 пакет, 1400 байт)

| Операция | Время | Доля CPU |
|----------|-------|----------|
| ChaCha20 init + XOR | ~400 нс | 4% |
| UDP syscall (read/write) | ~2000 нс | 40% |
| TUN syscall (read/write) | ~2000 нс | 40% |
| Session lookup (RLock) | ~100 нс | 1% |
| Прочее | ~1500 нс | 15% |

**Вывод:** 80% времени — системные вызовы (syscalls). Оптимизировать надо I/O.

---

## Уже выполненные оптимизации (фаза 0)

- [x] `ExtractDstIPKey` — zero-alloc извлечение dst IP как `[4]byte`
- [x] `GetSessionByIPKey` — zero-alloc lookup сессии по IP-ключу
- [x] `tunReadLoop` — интеграция zero-alloc lookup
- [x] `randomKeepaliveInterval` — убраны allocations `big.Int`
- [x] `randomCheckInterval` (клиент) — убраны `big.Int`
- [x] `keepaliveLoop` (клиент) — `time.After` → `time.Timer` (корректная очистка)
- [x] `monitorLoop` (клиент) — `time.After` → `time.Timer`

---

## Этап 1: Batch UDP отправка — sendmmsg (сервер) ✅

**Файлы:** `vpn-server/internal/server/server.go`, `vpn-server/internal/server/batch_linux.go`

**Проблема:** `tunReadLoop` отправляет пакеты по одному через `WriteToUDP` — 1 syscall / пакет.

**Решение:** Кольцевой буфер `batchSender` + `sendmmsg` — до 64 пакетов за 1 syscall.

**Реализовано:**
- `batchSender` — ring buffer с пре-аллоцированными буферами (64 слота)
- Flush goroutine с таймером 100 мкс + немедленный flush при заполнении
- `sendToClientBatch` — шифрование прямо в batch-буфер (zero-copy)
- Fallback на `WriteToUDP` если `getUDPSocketFd` не удался

**Влияние на маскировку:** нулевое (wire format не меняется).

**Статус:** ✅ завершён

---

## Этап 2: Batch UDP приём — recvmmsg (сервер) ✅

**Файлы:** `vpn-server/internal/server/server.go`, `vpn-server/internal/server/batch_linux.go`

**Проблема:** `udpReadLoop` читает по 1 пакету через `ReadFromUDP` — 1 syscall / пакет.

**Решение:** `recvmmsg` + `batchReceiver` читает до 64 пакетов за 1 syscall.

**Реализовано:**
- `batchReceiver` — пре-аллоцированные буферы, iovec, sockaddr для 64 пакетов
- `recvmmsg` syscall с `MSG_WAITFORONE` (возврат после первого пакета)
- `processUDPPacket` — извлечённый метод обработки одного пакета (batch + fallback)
- Fallback на `ReadFromUDP` если batch receiver не инициализирован

**Влияние на маскировку:** нулевое.

**Статус:** ✅ завершён

---

## Этап 3: Увеличение UDP буферов (сервер) ✅

**Файлы:** `vpn-server/internal/server/server.go`

**Проблема:** Стандартные буферы UDP-сокета (4 МБ) переполняются при burst'ах.

**Решение:** Fallback-стратегия: 16 МБ → 8 МБ → 4 МБ с логированием.

**Реализовано:**
- `tuneSocketBuffers()` — метод с fallback через 3 уровня (16/8/4 МБ)
- Логирование реально установленного размера буфера

**Влияние на маскировку:** нулевое.

**Статус:** ✅ завершён

---

## Этап 4: LRU-1 session cache (сервер) ✅

**Файлы:** `vpn-server/internal/server/session.go`, `vpn-server/internal/server/server.go`

**Проблема:** `GetSessionByID` и `GetSessionByIPKey` берут `RWMutex.RLock` на каждый пакет.

**Решение:** LRU-1 кеш вместо `sync.Map` (sync.Map создаёт interface boxing = аллокация на каждый пакет).

**Реализовано:**
- `sessionCache` — кеш последней сессии по SessionID для `udpReadLoop` (1 горутина)
- `ipSessionCache` — кеш последней сессии по VPN IP для `tunReadLoop` (1 горутина)
- При cache hit — нулевая стоимость (без RLock, без map lookup)
- Корректность: stale session обнаруживается через `IsActive()` (atomic state)
- Монотонный sessionID гарантирует отсутствие ID reuse

**Ожидаемый эффект:** устранение RLock на hot path для single-client (100% hit rate), высокий hit rate для multi-client (пакеты кластеризованы по сессиям).

**Влияние на маскировку:** нулевое.

**Статус:** ✅ завершён

---

## Этап 4.5: Семафор handshake — Argon2id throttling (сервер) ✅

**Файлы:** `vpn-server/internal/server/server.go`, `vpn-server/internal/server/handler.go`

**Проблема:** Argon2id аутентификация потребляет ~64 МБ памяти и ~500мс CPU на каждый handshake. При 50 одновременных подключениях: 50 × 64 МБ = 3.2 ГБ памяти + полное CPU starvation на VPS. Результат: 8/50 подключений.

**Решение:** Блокирующий семафор с таймаутом 15 секунд. Максимум 4 параллельных Argon2id (4 × 64 МБ = 256 МБ). Остальные ждут в очереди. При таймауте — `server_busy` ошибка.

**Реализовано:**
- `handshakeSem chan struct{}` (cap=4) в `VPNServer`
- `handleHandshakeInit()` — blocking select с `time.NewTimer(15s)`
- При переполнении очереди (>15с) → отправка `server_busy` ошибки
- Клиент с auto-reconnect повторит через backoff

**Результат:** 50/50 подключений (было 8/50). Handshake max = 6.87с (p95 = 6.72с).

**Влияние на маскировку:** нулевое.

**Статус:** ✅ завершён

---

## Этап 5: GRO/GSO для TUN (сервер, экспериментальный)

**Файлы:** `vpn-server/internal/tun/tun_linux.go`

**Проблема:** Каждый `Read`/`Write` на TUN — 1 пакет = 1 syscall. Linux 5.16+ поддерживает virtio GRO/GSO для TUN/TAP — до 64K данных за 1 syscall.

**Решение:** Активировать `IFF_VNET_HDR` + virtio header при создании TUN, реализовать batch read/write.

**Ожидаемый эффект:** 2-5x снижение TUN syscall overhead (вторые 40% нагрузки).

**Влияние на маскировку:** нулевое.

**Статус:** ⬜ не начат (требует Linux 5.16+, экспериментальный)

---

## Этап 6: MTU tuning + фрагментация (сервер + клиент)

**Файлы:** `vpn-server/config/config.go`, документация

**Проблема:** MTU = 1400 → на проводе 1400 + 14 (data overhead) + 8 (UDP) + 20 (IP) = **1442 байт**. Если ISP-путь имеет MTU < 1442 (PPPoE = 1492 OK, некоторые LTE/tunnels = 1440), каждый VPN-пакет фрагментируется → двойной syscall overhead + потери.

**Решение:**
1. Снизить MTU до 1380 по умолчанию (1380+14+8+20 = 1422 — безопасно даже для PPPoE)
2. Добавить конфигурируемый параметр `mtu_discovery: auto|manual`
3. Документировать рекомендации

**Ожидаемый эффект:** устранение фрагментации = x2 выигрыш для пользователей с узким MTU.

**Влияние на маскировку:** нулевое.

**Статус:** ⬜ не начат

---

## Этап 7: Оптимизации клиента (Windows)

**Файлы:** `vpn-client-windows/internal/infrastructure/vpn/client.go`, `crypto/session.go`

**Проблема:** Мелкие аллокации на hot path клиента.

**Решение:**
1. `udpReadLoop` — увеличить буфер чтения с 2048 до MTU+DataOverhead+100
2. `DecryptWithCounter` — проверить reuse буфера `recvBuf` (уже сделано, OK)
3. `WinTUNDevice.Read` — WaitForSingleObject таймаут 100мс → 1000мс (меньше wakeup'ов в idle)

**Ожидаемый эффект:** незначительный (~5%), но снижает CPU usage в idle.

**Влияние на маскировку:** нулевое.

**Статус:** ⬜ не начат

---

## Порядок реализации

```
Этап 0 ✅  Zero-alloc hot path
  │
  ▼
Этап 1 ✅  sendmmsg batch sender
  │
  ▼
Этап 2 ✅  recvmmsg batch receiver + processUDPPacket
  │
  ▼
Этап 3 ✅  UDP socket buffer tuning (16/8/4 МБ fallback)
  │
  ▼
Этап 4 ✅  LRU-1 session cache (sessionCache + ipSessionCache)
  │
  ▼
Этап 4.5 ✅  Семафор handshake (Argon2id throttling, 4 параллельных)
  │
  ▼  
Этап 5 ⬜  GRO/GSO TUN (экспериментальный, высокий эффект)
  │
  ▼
Этап 6 ⬜  MTU tuning (конфигурация, документация)
  │
  ▼
Этап 7 ⬜  Клиентские мелочи (низкий приоритет)
```

---

## Метрики успеха

| Метрика | Текущее | Цель |
|---------|---------|------|
| Syscalls на 10K pps (send) | 10,000 | < 500 |
| Syscalls на 10K pps (recv) | 10,000 | < 500 |
| Аллокации на data-пакет | 0-1 | 0 |
| RLock contention при 10 клиентах | заметная | нулевая |
| Throughput (1 клиент) | ~300-500 Mbps* | ~800+ Mbps |
| Throughput (10 клиентов) | ~200-300 Mbps* | ~600+ Mbps |

*оценка, зависит от сервера

---

## Бенчмарк-инструмент (vpnbench) ✅

**Файлы:** `vpn-server/cmd/vpnbench/main.go`, `vpn-server/cmd/vpnbench/client.go`

**Назначение:** Нагрузочное тестирование VPN-сервера с конкретными метриками для сравнения реализаций.

### Возможности

- N одновременных VPN-клиентов (полный handshake + data exchange)
- Keepalive ping-pong для измерения RTT
- Метрики: handshake latency, RTT (min/avg/p50/p95/p99/max), throughput, потери пакетов, ошибки
- JSON-отчёт для автоматизации и сравнения
- Кросс-компиляция для запуска на сервере (localhost-тест)

### Использование

```bash
# Сборка (кросс-компиляция для Linux)
GOOS=linux GOARCH=amd64 go build -o vpnbench ./cmd/vpnbench/

# RTT режим (keepalive ping-pong)
./vpnbench \
  -server 127.0.0.1:443 \
  -psk <hex64> \
  -email test@novavpn.app \
  -password NovaVPN2026! \
  -clients 10 \
  -duration 15s \
  -interval 100ms \
  -mode rtt \
  -json результат.json

# Throughput режим (data flood)
./vpnbench \
  -server 127.0.0.1:443 \
  -psk <hex64> \
  -email test@novavpn.app \
  -password NovaVPN2026! \
  -clients 1 \
  -duration 15s \
  -mode throughput \
  -json результат.json
```

### Скрипт запуска с ноутбука

Скрипт `vpn-server/deploy/bench.ps1` автоматически компилирует, деплоит и запускает серию тестов на сервере с итоговой сводкой, оценками и сравнением с другими VPN-протоколами.

```powershell
# Полный набор (5 тестов: throughput×1, throughput×10, rtt×1, rtt×10, rtt×50)
.\bench.ps1

# Быстрый режим (3 теста: throughput×1, rtt×1, rtt×10)
.\bench.ps1 -quick

# С другой длительностью
.\bench.ps1 -duration 30s
```

### Базовые результаты (14.02.2026, сервер localhost, этапы 0-4.5)

**Throughput: 1 клиент, 15 сек, data flood (1000 байт пакеты):**

| Метрика | Значение |
|---------|----------|
| Handshake | 234 мс |
| Пакетов/сек | 63,589 |
| Throughput | **508.71 Мбит/с** |

**RTT: 10 клиентов, 15 сек, keepalive каждые 100мс:**

| Метрика | Значение |
|---------|----------|
| Handshake (avg) | 1.45 сек |
| Handshake (min/max) | 607мс / 2.07с |
| RTT (avg) | 14.3 мс |
| RTT (p50) | 132 мкс |
| RTT (p95) | 574 мкс |
| RTT (p99) | 559 мс |
| Потери | 0.00% |
| Успешные подключения | 10/10 |

**RTT: 50 клиентов, 30 сек (с семафором handshake):**

| Метрика | Значение |
|---------|----------|
| Успешные подключения | **50/50** ✅ (было 8/50) |
| Handshake (avg) | 3.64 сек |
| Handshake (min/max) | 180мс / 6.87с |
| Handshake (p95) | 6.72 сек |
| RTT (avg) | 35.6 мс |
| RTT (p50) | 105 мкс |
| RTT (p95) | 309 мс |
| RTT (p99) | 537 мс |
| Потери | 0.00% |

**Вывод:** Семафор handshake (этап 4.5) решил проблему массовых подключений: 50/50 вместо 8/50. Пропускная способность сервера — 508 Мбит/с на localhost (1 клиент).

